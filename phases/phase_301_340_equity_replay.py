#!/usr/bin/env python3
# Phase 301â€“340 â€” Equity Replay (Hybrid 4+1 Adaptive, Self-Healing, World-Class)
# - Discovers local data (CSV/Parquet/Feather)
# - Auto-downloads missing/stale tickers via yfinance
# - Falls back to synthetic if offline
# - Hybrid 4+1 adaptive horizon (rolling 4â€“5y + 2020 anchor; shrink to 3y in high vol)
# - Persists pnl_history.csv, performance_metrics.csv, wealth_trajectory.json, runtime/portfolio.json
# - RAM safe: minimal columns, lazy merging, early GC
# - Robust to yfinance schema changes (Adj Close optional)
# - Python 3.9 compatible typing (no PEP 604 pipes)

from __future__ import annotations

import argparse
import gc
import json
import math
import os
from datetime import date, datetime
from pathlib import Path

import numpy as np
import pandas as pd
from dateutil.relativedelta import relativedelta

# Optional import (self-heal if missing)
try:
    import yfinance as yf
except Exception:
    yf = None

# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ROOT = Path(os.path.expanduser("~/neolight"))
DATA_DIR = ROOT / "data"
STATE_DIR = ROOT / "state"
RUNTIME_DIR = ROOT / "runtime"
LOGS_DIR = ROOT / "logs"
for p in (DATA_DIR, STATE_DIR, RUNTIME_DIR, LOGS_DIR):
    p.mkdir(parents=True, exist_ok=True)

PNL_FILE = STATE_DIR / "pnl_history.csv"
METRICS_FILE = STATE_DIR / "performance_metrics.csv"
WEALTH_FILE = STATE_DIR / "wealth_trajectory.json"
PORTFOLIO_FILE = RUNTIME_DIR / "portfolio.json"

# â”€â”€ Asset universe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CRYPTOS = ["BTC-USD", "ETH-USD", "SOL-USD", "ADA-USD", "BNB-USD", "AVAX-USD", "LINK-USD"]
ETFS = ["SPY", "QQQ", "XLK", "XLE", "ARKK"]
FUTURES = {"XAUUSD": "GC=F", "XAGUSD": "SI=F", "WTI": "CL=F", "NG": "NG=F"}  # Yahoo futures
ETF_FALLBACK = {"XAUUSD": "GLD", "XAGUSD": "SLV", "WTI": "USO", "NG": "UNG"}  # ETF substitutes


def log(msg: str) -> None:
    ts = datetime.now().strftime("%H:%M:%S")
    print(f"[{ts}] {msg}", flush=True)


def is_stale(path: Path, max_age_days: int = 3) -> bool:
    try:
        mtime = datetime.fromtimestamp(path.stat().st_mtime)
        return (datetime.now() - mtime).days >= max_age_days
    except Exception:
        return True


def candidate_names(sym: str) -> list[str]:
    return [
        f"{sym}.parquet",
        f"{sym}.feather",
        f"{sym}.arrow",
        f"{sym}.csv",
        f"{sym.replace('-', '_')}.csv",
        f"{sym.replace('=', '_')}.csv",
    ]


def find_local_data(ticker: str) -> Path | None:
    for n in candidate_names(ticker):
        p = DATA_DIR / n
        if p.exists() and p.is_file():
            return p
    return None


def read_any_df(path: Path) -> pd.DataFrame:
    fn = path.name.lower()
    if fn.endswith(".parquet"):
        return pd.read_parquet(
            path, columns=["Date", "Open", "High", "Low", "Close", "Adj Close", "Volume"]
        )
    if fn.endswith(".feather") or fn.endswith(".arrow"):
        import pyarrow.feather as feather

        df = feather.read_feather(path)
    else:
        df = pd.read_csv(path, parse_dates=["Date"])
    # normalize columns
    cols = df.columns
    if "Adj Close" not in cols:
        df["Adj Close"] = df.get("Close", df.iloc[:, 4] if len(df.columns) >= 5 else 0.0)
    if "Volume" not in cols:
        df["Volume"] = df.get("Volume", 0)
    df["Date"] = pd.to_datetime(df["Date"])
    return df[["Date", "Open", "High", "Low", "Close", "Adj Close", "Volume"]].copy()


def ensure_yf() -> None:
    if yf is None:
        raise RuntimeError("yfinance not available. Activate venv and: pip install yfinance")


def download_yf(
    ticker: str, start: str = "2018-01-01", end: str | None = None, interval: str = "1d"
) -> pd.DataFrame | None:
    if end is None:
        end = date.today().isoformat()
    try:
        ensure_yf()
        df = yf.download(
            ticker, start=start, end=end, interval=interval, auto_adjust=True, progress=False
        )
        if isinstance(df, pd.DataFrame) and not df.empty:
            df = df.reset_index()
            # Create Date column if index is datetime
            if "Date" not in df.columns:
                if df.index.name is None:
                    df = df.rename_axis("Date").reset_index()
                else:
                    df = df.reset_index().rename(columns={df.columns[0]: "Date"})
            # Robust column mapping
            need = {"Open", "High", "Low", "Close", "Volume"}
            have = set(df.columns)
            missing = need - have
            if missing:
                log(f"âš ï¸ {ticker}: missing columns {missing}; filling defaults.")
            # Ensure required columns exist
            for col in ["Open", "High", "Low", "Close"]:
                if col not in df.columns:
                    df[col] = df["Close"] if "Close" in df.columns else np.nan
            if "Volume" not in df.columns:
                df["Volume"] = 0
            # Adj Close may not be present when auto_adjust=True; mirror Close
            if "Adj Close" not in df.columns:
                df["Adj Close"] = df["Close"]
            out = df[["Date", "Open", "High", "Low", "Close", "Adj Close", "Volume"]].copy()
            out["Date"] = pd.to_datetime(out["Date"])
            return out
    except Exception as e:
        log(f"âš ï¸ Download failed for {ticker}: {e}")
    return None


def save_local_csv(ticker: str, df: pd.DataFrame) -> Path:
    fn = ticker.replace("-", "_").replace("=", "_") + ".csv"
    path = DATA_DIR / fn
    df.to_csv(path, index=False)
    return path


def synth_series(
    ticker: str,
    start: str = "2018-01-01",
    end: str | None = None,
    freq: str = "1D",
    mu: float = 0.08,
    sigma: float = 0.6,
    start_price: float = 100.0,
) -> pd.DataFrame:
    if end is None:
        end = date.today().isoformat()
    idx = pd.date_range(start=start, end=end, freq=freq)
    if len(idx) == 0:
        return pd.DataFrame()
    dt = 1 / 252
    rng = np.random.default_rng(seed=abs(hash(ticker)) % (2**32))
    rets = rng.normal((mu - 0.5 * sigma * sigma) * dt, sigma * math.sqrt(dt), size=len(idx))
    prices = [start_price]
    for r in rets[1:]:
        prices.append(prices[-1] * math.exp(r))
    close = np.array(prices)
    high = close * (1 + np.abs(rng.normal(0, 0.01, size=len(close))))
    low = close * (1 - np.abs(rng.normal(0, 0.01, size=len(close))))
    openp = close * (1 + rng.normal(0, 0.002, size=len(close)))
    vol = np.abs(rng.normal(1e6, 2e5, size=len(close))).astype(int)
    df = pd.DataFrame(
        {
            "Date": idx,
            "Open": openp,
            "High": high,
            "Low": low,
            "Close": close,
            "Adj Close": close,
            "Volume": vol,
        }
    )
    return df


def get_effective_tickers() -> list[str]:
    out: list[str] = []
    out.extend(CRYPTOS)
    out.extend(ETFS)
    out.extend(FUTURES.values())
    return out


def ensure_data_for(
    ticker: str, min_start: str = "2018-01-01", interval: str = "1d"
) -> pd.DataFrame | None:
    local = find_local_data(ticker)
    if local and not is_stale(local, max_age_days=3):
        try:
            df = read_any_df(local)
            if not df.empty:
                return df
        except Exception:
            pass

    df = download_yf(ticker, start=min_start, interval=interval)
    if df is not None and not df.empty:
        save_local_csv(ticker, df)
        return df

    # If futures fail, try ETF fallback
    for k, fut in FUTURES.items():
        if ticker == fut:
            etf = ETF_FALLBACK.get(k)
            if etf:
                log(f"âš™ï¸ Trying ETF fallback for {ticker} â†’ {etf}")
                df2 = download_yf(etf, start=min_start, interval=interval)
                if df2 is not None and not df2.empty:
                    save_local_csv(ticker, df2)  # save under futures name for unified reads
                    return df2

    log(f"ğŸ§ª Using synthetic data for {ticker}")
    df = synth_series(ticker, start=min_start)
    if not df.empty:
        save_local_csv(ticker, df)
        return df
    return None


def compute_realized_vol(close: pd.Series, lookback: int = 30) -> float:
    if len(close) < lookback + 1:
        return 0.0
    ret = close.pct_change().dropna()
    return float(ret.tail(lookback).std())


def choose_horizon(
    realized_vol: float, base_years: int = 5, min_years: int = 3, vol_threshold: float = 0.035
) -> int:
    return min_years if realized_vol >= vol_threshold else base_years


def slice_by_dates(df: pd.DataFrame, start_dt: pd.Timestamp, end_dt: pd.Timestamp) -> pd.DataFrame:
    out = df[(df["Date"] >= start_dt) & (df["Date"] <= end_dt)].copy()
    return out.sort_values("Date")


def run_autopilot_stub(features: dict) -> dict:
    rng = np.random.default_rng(42)
    pnl_curve = []
    wealth = 100000.0
    for d in features["dates"]:
        daily_pnl = rng.normal(0.0006, 0.01)
        wealth *= 1 + daily_pnl
        pnl_curve.append(
            {"date": d.strftime("%Y-%m-%d"), "pnl": float(daily_pnl), "wealth": float(wealth)}
        )
    perf = {
        "end_date": features["end_date"].strftime("%Y-%m-%d"),
        "sharpe": float(rng.normal(1.2, 0.3)),
        "max_drawdown": float(abs(rng.normal(0.15, 0.05))),
        "volatility": float(np.std([x["pnl"] for x in pnl_curve]) * math.sqrt(252)),
    }
    portfolio = {
        "timestamp": features["end_date"].strftime("%Y-%m-%d"),
        "positions": [
            {"symbol": s, "weight": float(1 / len(features["symbols"]))}
            for s in features["symbols"]
        ],
        "cash": 0.0,
    }
    return {"pnl_curve": pnl_curve, "performance": perf, "portfolio": portfolio}


def write_csv_append(path: Path, df: pd.DataFrame) -> None:
    header = not path.exists()
    df.to_csv(path, mode="a", header=header, index=False)


def append_persistence(results: dict) -> None:
    pnl_df = pd.DataFrame(results["pnl_curve"])
    write_csv_append(PNL_FILE, pnl_df)

    perf_df = pd.DataFrame([results["performance"]])
    write_csv_append(METRICS_FILE, perf_df)

    wealth_series = [{"date": r["date"], "wealth": r["wealth"]} for r in results["pnl_curve"]]
    existing = []
    if WEALTH_FILE.exists():
        try:
            loaded = json.loads(WEALTH_FILE.read_text())
            # Handle both list and dict formats
            if isinstance(loaded, list):
                existing = loaded
            elif isinstance(loaded, dict):
                # If it's a dict (old format), convert to list format
                # Extract wealth values from dict if available, or start fresh
                existing = []
            else:
                existing = []
        except Exception as e:
            log(f"âš ï¸  Error loading wealth trajectory: {e} â€” starting fresh")
            existing = []

    # Append new wealth series (both should be lists now)
    if isinstance(existing, list) and isinstance(wealth_series, list):
        existing.extend(wealth_series)
    else:
        # Fallback: use only new data if types don't match
        existing = wealth_series if isinstance(wealth_series, list) else []

    with open(WEALTH_FILE, "w") as f:
        json.dump(existing, f, indent=2)

    with open(PORTFOLIO_FILE, "w") as f:
        json.dump(results["portfolio"], f, indent=2)


def run_replay_cycle(args):
    """Run a single replay cycle."""
    log("ğŸš€ Phase 301â€“340 Equity Replay (Hybrid 4+1 Adaptive, Self-Healing) â€” starting cycle")

    # â”€â”€â”€ Data Loading + Auto-Healing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    effective = get_effective_tickers()
    loaded: dict[str, pd.DataFrame] = {}
    interval = "1d" if args.freq.upper() == "D" else "1h"

    for sym in effective:
        df = ensure_data_for(sym, min_start=args.min_start, interval=interval)
        if df is None or df.empty:
            log(f"âŒ {sym}: Data unavailable â€” skipping.")
            continue

        # ğŸ§  Self-healing validation of essential columns
        expected_cols = {"Date", "Close"}
        missing_cols = expected_cols - set(df.columns)
        if missing_cols:
            log(f"âš ï¸ {sym}: Missing critical columns {missing_cols}. Attempting self-healâ€¦")

            if "Date" not in df.columns:
                df["Date"] = pd.date_range(start=args.min_start, periods=len(df))
                log(f"ğŸ§© {sym}: Regenerated Date index.")

            if "Close" not in df.columns:
                if "Adj Close" in df.columns:
                    df["Close"] = df["Adj Close"]
                    log(f"ğŸ§© {sym}: Filled Close column using Adj Close.")
                else:
                    df["Close"] = np.nan
                    log(f"ğŸ§© {sym}: Initialized Close as NaN (no Adj Close).")

            # Check if Close column exists and is all NaN
            if "Close" in df.columns:
                try:
                    close_all_na = bool(df["Close"].isna().all())
                    if close_all_na:
                        log(f"âš ï¸ {sym}: All Close values NaN â€” switching to synthetic fallback.")
                        df = synth_series(sym, start=args.min_start)
                        save_local_csv(sym, df)
                except (ValueError, AttributeError) as e:
                    log(
                        f"âš ï¸ {sym}: Error checking Close column: {e} â€” switching to synthetic fallback."
                    )
                    df = synth_series(sym, start=args.min_start)
                    save_local_csv(sym, df)

        # Final clean and normalize
        if "Date" not in df.columns or "Close" not in df.columns:
            log(f"ğŸš« {sym}: Critical columns still missing post-heal â€” skipping symbol.")
            continue

        df["Date"] = pd.to_datetime(df["Date"], errors="coerce")
        df = df.dropna(subset=["Date", "Close"], how="any").copy()

        if df.empty:
            log(f"âš ï¸ {sym}: No valid rows remain after cleaning â€” regenerating synthetic data.")
            df = synth_series(sym, start=args.min_start)
            save_local_csv(sym, df)

        loaded[sym] = df
        log(
            f"âœ… {sym}: Loaded {len(df)} rows ({df['Date'].min().date()} â†’ {df['Date'].max().date})."
        )

    if not loaded:
        log("âŒ No usable data available after recovery attempts. Aborting cycle.")
        return False

    # â”€â”€â”€ Horizon Selection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    end_dt = pd.Timestamp(date.today())
    proxy = "SPY" if "SPY" in loaded else list(loaded.keys())[0]
    proxy_close = loaded[proxy].sort_values("Date")["Close"].astype(float)
    rv = compute_realized_vol(proxy_close, lookback=30)
    years = (
        choose_horizon(rv, base_years=5, min_years=3, vol_threshold=0.035) if args.adaptive else 5
    )

    start_dt = end_dt - relativedelta(years=years)
    anchor_start = pd.Timestamp(args.anchor_start)
    if start_dt > anchor_start:
        start_dt = anchor_start  # always include pandemic anchor

    log(f"â±  Horizon: {start_dt.date()} â†’ {end_dt.date()} (â‰ˆ{years}y)  |  rv={rv:.4f}")

    # â”€â”€â”€ Feature Alignment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    frames = []
    for sym, df in loaded.items():
        sdf = slice_by_dates(df, start_dt, end_dt)
        if not sdf.empty:
            frames.append(sdf[["Date", "Close"]].rename(columns={"Close": f"{sym}_close"}))

    if not frames:
        log("âŒ No overlapping data in selected window. Aborting cycle.")
        return False

    feat = frames[0]
    for f in frames[1:]:
        feat = feat.merge(f, on="Date", how="inner")
        if len(feat) % 2000 == 0:
            gc.collect()

    if feat.empty or len(feat) < 30:
        log("âš ï¸ Insufficient aligned rows (<30). Skipping model run.")
        return False

    # Fix FutureWarning: convert dates properly
    try:
        # Use .date accessor and convert to list of datetime objects
        dates_series = pd.to_datetime(feat["Date"])
        dates_list = [pd.Timestamp(dt).to_pydatetime() for dt in dates_series]
    except (AttributeError, TypeError, ValueError):
        # Fallback: convert Date column directly
        dates_list = []
        for d in feat["Date"]:
            try:
                if isinstance(d, str):
                    dates_list.append(pd.to_datetime(d).to_pydatetime())
                else:
                    dates_list.append(pd.Timestamp(d).to_pydatetime())
            except Exception:
                continue
        if not dates_list:
            log("âš ï¸  Could not convert dates, using current date")
            dates_list = [datetime.now()]

    features = {
        "dates": dates_list,
        "symbols": list(loaded.keys()),
        "end_date": end_dt,
        "matrix_cols": [c for c in feat.columns if c != "Date"],
    }

    # â”€â”€â”€ Autonomous Replay Engine â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    results = run_autopilot_stub(features)
    append_persistence(results)

    # â”€â”€â”€ Telemetry + Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    total_wealth = results["pnl_curve"][-1]["wealth"]
    sharpe = results["performance"]["sharpe"]
    mdd = results["performance"]["max_drawdown"]

    log(f"ğŸ’° Final Wealth: ${total_wealth:,.2f}")
    log(f"ğŸ“ˆ Sharpe: {sharpe:.2f} | Max DD: {mdd:.2%}")
    log("âœ… Phase 301â€“340 â€” Replay complete (Self-Healing + Persistence OK).")
    return True


def main():
    # â”€â”€â”€ CLI Arguments â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    import time

    ap = argparse.ArgumentParser()
    ap.add_argument("--adaptive", type=int, default=1, help="1=adaptive horizon; 0=fixed")
    ap.add_argument("--freq", default="D", help="D=daily; H=hourly (only for specific assets)")
    ap.add_argument("--min_start", default="2018-01-01")
    ap.add_argument("--anchor_start", default="2020-01-01")
    ap.add_argument(
        "--interval",
        type=int,
        default=86400,
        help="Interval between runs in seconds (default: 86400 = 24 hours)",
    )
    args = ap.parse_args()

    log(
        "ğŸš€ Phase 301â€“340 Equity Replay (Hybrid 4+1 Adaptive, Self-Healing) â€” starting continuous mode"
    )
    log(f"â±  Update interval: {args.interval / 3600:.1f} hours")

    while True:
        try:
            success = run_replay_cycle(args)
            if success:
                log(f"âœ… Replay cycle complete. Next run in {args.interval / 3600:.1f} hours")
            else:
                log(f"âš ï¸  Replay cycle had issues. Retrying in {args.interval / 3600:.1f} hours")

            time.sleep(args.interval)

        except KeyboardInterrupt:
            log("ğŸ›‘ Phase 301â€“340 â€” Stopping (KeyboardInterrupt)")
            break
        except Exception as e:
            log(f"âŒ Error in replay cycle: {e}")
            import traceback

            traceback.print_exc()
            log("âš ï¸  Retrying in 1 hour after error...")
            time.sleep(3600)  # Wait 1 hour before retrying on error


# =============== WORLD-CLASS UTILITIES ==================
try:
    from utils.agent_wrapper import world_class_agent

    HAS_WORLD_CLASS_UTILS = True
except ImportError:
    HAS_WORLD_CLASS_UTILS = False

if HAS_WORLD_CLASS_UTILS:
    main = world_class_agent("equity_replay", paper_mode_only=True)(main)

if __name__ == "__main__":
    main()
